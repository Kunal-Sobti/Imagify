{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab166c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\Kunal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Kunal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 73\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kunal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[0m\n\u001b[0;32m     37\u001b[0m _os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\Kunal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[0m\n\u001b[0;32m     86\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     94\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\Kunal\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import cv2\n",
    "import PIL\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#downloading dataset:\n",
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\n",
    "\n",
    "#Data processing:\n",
    "def normalize(input_image, input_mask):\n",
    "  \n",
    "    # Normalize the pixel range values between [0:1]\n",
    "    img = tf.cast(input_image, dtype=tf.float32) / 255.0\n",
    "    input_mask -= 1\n",
    "    return img, input_mask\n",
    "\n",
    "@tf.function\n",
    "def load_train_ds(dataset):\n",
    "    img = tf.image.resize(dataset['image'], \n",
    "                          size=(width, height))\n",
    "    mask = tf.image.resize(dataset['segmentation_mask'],\n",
    "                           size=(width, height))\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        img = tf.image.flip_left_right(img)\n",
    "        mask = tf.image.flip_left_right(mask)\n",
    "\n",
    "    img, mask = normalize(img, mask)\n",
    "    return img, mask\n",
    "\n",
    "@tf.function\n",
    "def load_test_ds(dataset):\n",
    "    img = tf.image.resize(dataset['image'], \n",
    "                          size=(width, height))\n",
    "    mask = tf.image.resize(dataset['segmentation_mask'], \n",
    "                           size=(width, height))\n",
    "\n",
    "    img, mask = normalize(img, mask)\n",
    "    return img, mask\n",
    "\n",
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "\n",
    "# Batch size is the number of examples used in one training example.\n",
    "# It is mostly a power of 2\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
    "\n",
    "# For VGG16 this is the input size\n",
    "width, height = 224, 224\n",
    "\n",
    "train = dataset['train'].map(\n",
    "    load_train_ds, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test = dataset['test'].map(load_test_ds)\n",
    "\n",
    "train_ds = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "test_ds = test.batch(BATCH_SIZE)\n",
    "\n",
    "#Visualizing data:\n",
    "def display_images(display_list):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    title = ['Input Image', 'True Mask', \n",
    "             'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for img, mask in train.take(1):\n",
    "    sample_image, sample_mask = img, mask\n",
    "    display_list = sample_image, sample_mask\n",
    "\n",
    "display_images(display_list)\n",
    "\n",
    "#U-Net:\n",
    "base_model = keras.applications.vgg16.VGG16(\n",
    "    include_top=False, input_shape=(width, height, 3))\n",
    "\n",
    "layer_names = [\n",
    "    'block1_pool',\n",
    "    'block2_pool',\n",
    "    'block3_pool',\n",
    "    'block4_pool',\n",
    "    'block5_pool',\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(\n",
    "    name).output for name in layer_names]\n",
    "base_model.trainable = False\n",
    "\n",
    "VGG_16 = tf.keras.models.Model(base_model.input,\n",
    "                               base_model_outputs)\n",
    "\n",
    "#defined decoder:\n",
    "def fcn8_decoder(convs, n_classes):\n",
    "    f1, f2, f3, f4, p5 = convs\n",
    "\n",
    "    n = 4096\n",
    "    c6 = tf.keras.layers.Conv2D(\n",
    "        n, (7, 7), activation='relu', padding='same', \n",
    "      name=\"conv6\")(p5)\n",
    "    c7 = tf.keras.layers.Conv2D(\n",
    "        n, (1, 1), activation='relu', padding='same', \n",
    "      name=\"conv7\")(c6)\n",
    "\n",
    "    f5 = c7\n",
    "\n",
    "    # upsample the output of the encoder\n",
    "    # then crop extra pixels that were introduced\n",
    "    o = tf.keras.layers.Conv2DTranspose(n_classes, kernel_size=(\n",
    "        4, 4),  strides=(2, 2), use_bias=False)(f5)\n",
    "    o = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n",
    "\n",
    "    # load the pool 4 prediction and do a 1x1\n",
    "    # convolution to reshape it to the same shape of `o` above\n",
    "    o2 = f4\n",
    "    o2 = (tf.keras.layers.Conv2D(n_classes, (1, 1),\n",
    "                                 activation='relu', \n",
    "                                 padding='same'))(o2)\n",
    "\n",
    "    # add the results of the upsampling and pool 4 prediction\n",
    "    o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "    # upsample the resulting tensor of the operation you just did\n",
    "    o = (tf.keras.layers.Conv2DTranspose(\n",
    "        n_classes, kernel_size=(4, 4),  strides=(2, 2), \n",
    "      use_bias=False))(o)\n",
    "    o = tf.keras.layers.Cropping2D(cropping=(1, 1))(o)\n",
    "\n",
    "    # load the pool 3 prediction and do a 1x1\n",
    "    # convolution to reshape it to the same shape of `o` above\n",
    "    o2 = f3\n",
    "    o2 = (tf.keras.layers.Conv2D(n_classes, (1, 1),\n",
    "                                 activation='relu', \n",
    "                                 padding='same'))(o2)\n",
    "\n",
    "    # add the results of the upsampling and pool 3 prediction\n",
    "    o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "    # upsample up to the size of the original image\n",
    "    o = tf.keras.layers.Conv2DTranspose(\n",
    "        n_classes, kernel_size=(8, 8),  strides=(8, 8),\n",
    "      use_bias=False)(o)\n",
    "\n",
    "    # append a softmax to get the class probabilities\n",
    "    o = tf.keras.layers.Activation('softmax')(o)\n",
    "    return o\n",
    "\n",
    "#combining everything:\n",
    "def segmentation_model():\n",
    "\n",
    "    inputs = keras.layers.Input(shape=(width, height, 3))\n",
    "    convs = VGG_16(inputs)\n",
    "    outputs = fcn8_decoder(convs, 3)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "model = segmentation_model()\n",
    "model.compile(optimizer=opt,\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                  from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Creating prediction mask utility:\n",
    "def create_mask(pred_mask):\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "    pred_mask = pred_mask[..., tf.newaxis]\n",
    "    return pred_mask[0]\n",
    "\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display_images([image[0], mask[0], create_mask(pred_mask)])\n",
    "    else:\n",
    "        display_images([sample_image, sample_mask,\n",
    "                        create_mask(model.predict(sample_image[tf.newaxis, ...]))])\n",
    "\n",
    "\n",
    "show_predictions()\n",
    "\n",
    "#Training\n",
    "EPOCHS = 20\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_ds, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_ds)\n",
    "\n",
    "#Metrics:\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    '''\n",
    "    Computes IOU and Dice Score.\n",
    "\n",
    "    Args:\n",
    "      y_true (tensor) - ground truth label map\n",
    "      y_pred (tensor) - predicted label map\n",
    "    '''\n",
    "\n",
    "    class_wise_iou = []\n",
    "    class_wise_dice_score = []\n",
    "\n",
    "    smoothening_factor = 0.00001\n",
    "\n",
    "    for i in range(3):\n",
    "        intersection = np.sum((y_pred == i) * (y_true == i))\n",
    "        y_true_area = np.sum((y_true == i))\n",
    "        y_pred_area = np.sum((y_pred == i))\n",
    "        combined_area = y_true_area + y_pred_area\n",
    "\n",
    "        iou = (intersection + smoothening_factor) / \\\n",
    "            (combined_area - intersection + smoothening_factor)\n",
    "        class_wise_iou.append(iou)\n",
    "\n",
    "        dice_score = 2 * ((intersection + smoothening_factor) /\n",
    "                          (combined_area + smoothening_factor))\n",
    "        class_wise_dice_score.append(dice_score)\n",
    "\n",
    "    return class_wise_iou, class_wise_dice_score\n",
    "\n",
    "#model prediction with metrics:\n",
    "def get_test_image_and_annotation_arrays():\n",
    "    '''\n",
    "    Unpacks the test dataset and returns\n",
    "    the input images and segmentation masks\n",
    "    '''\n",
    "\n",
    "    ds = test_ds.unbatch()\n",
    "    ds = ds.batch(info.splits['test'].num_examples)\n",
    "\n",
    "    images = []\n",
    "    y_true_segments = []\n",
    "\n",
    "    for image, annotation in ds.take(1):\n",
    "        y_true_segments = annotation.numpy()\n",
    "        images = image.numpy()\n",
    "\n",
    "    y_true_segments = y_true_segments[:(\n",
    "        info.splits['test'].num_examples - (info.splits['test']\n",
    "                                            .num_examples % BATCH_SIZE))]\n",
    "    images = images[:(info.splits['test'].num_examples -\n",
    "                      (info.splits['test'].num_examples % BATCH_SIZE))]\n",
    "\n",
    "    return images, y_true_segments\n",
    "\n",
    "\n",
    "y_true_images, y_true_segments = get_test_image_and_annotation_arrays()\n",
    "\n",
    "integer_slider = 2574\n",
    "img = np.reshape(y_true_images[integer_slider], (1, width, height, 3))\n",
    "y_pred_mask = model.predict(img)\n",
    "y_pred_mask = create_mask(y_pred_mask)\n",
    "y_pred_mask.shape\n",
    "\n",
    "\n",
    "def display_prediction(display_list, display_string):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        if i == 1:\n",
    "            plt.xlabel(display_string, fontsize=12)\n",
    "        plt.imshow(keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "iou, dice_score = compute_metrics(\n",
    "    y_true_segments[integer_slider], y_pred_mask.numpy())\n",
    "display_list = [y_true_images[integer_slider],\n",
    "                y_true_segments[integer_slider], y_pred_mask]\n",
    "\n",
    "display_string_list = [\"{}: IOU: {} Dice Score: {}\".format(class_names[idx],\n",
    "                                                           i, dc) for idx, (i, dc) in\n",
    "                       enumerate(zip(np.round(iou, 4), np.round(dice_score, 4)))]\n",
    "display_string = \"\\n\\n\".join(display_string_list)\n",
    "\n",
    "\n",
    "# showing predictions with metrics\n",
    "display_prediction(display_list, display_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b537e77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
